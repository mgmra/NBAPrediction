{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import statsmodels.api as sm \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import pprint as pp\n",
    "import itertools\n",
    "from collections import ChainMap\n",
    "from sklearn.model_selection import cross_validate\n",
    "import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('Data/preprocessed/df_ml_hdf.h5', key='match')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "def preselect_non_redundant_features(df, target, corr_threshold):\n",
    "    corr = df.corr()\n",
    "    corr[target] = abs(corr[target])\n",
    "    corr = corr.sort_values(target, ascending=False)\n",
    "\n",
    "    #prepare lower trainagular matrix (with deleted diagonal)\n",
    "    low_tri_no_diag = np.tril(np.ones(corr.shape).astype(np.bool), k=-1)\n",
    "    corr = corr.where(low_tri_no_diag)\n",
    "    #drop tagret variable form rows and columns\n",
    "    corr = corr.drop(corr.columns[0], axis=1).drop(corr.index[0], axis=0)\n",
    "\n",
    "    corr['MaxCorr'] = corr.abs().max(axis = 1)\n",
    "\n",
    "    pre_selected_features = corr[corr['MaxCorr'] < corr_threshold].index \n",
    "    return pre_selected_features\n",
    "    \n",
    "def get_non_redundant_df(df, target, corr_threshold):\n",
    "    features = preselect_non_redundant_features(df, target, corr_threshold)\n",
    "    output = df.loc[:,features]\n",
    "    return output\n",
    "\n",
    "\n",
    "def feature_based_train_test_split(df, target, splitting_feature, test_split_value, \n",
    "                                   splitting_feature_as_index=True):\n",
    "    if splitting_feature_as_index == True:\n",
    "        X_train = df[~df.index.isin([test_split_value], level = splitting_feature)]\n",
    "        X_test = df[df.index.isin([test_split_value], level = splitting_feature)]\n",
    "                                  \n",
    "        y_train = df[~df.index.isin([test_split_value], level = splitting_feature)].reset_index()[target].values\n",
    "        y_test = df[df.index.isin([test_split_value], level = splitting_feature)].reset_index()[target].values\n",
    "                   \n",
    "    else:\n",
    "        X_train = df[~df[splitting_feature]==test_split_value]\n",
    "        X_test = df[df[splitting_feature]==test_split_value]\n",
    "        \n",
    "        y_train = df[~df[splitting_feature]==test_split_value].reset_index()[target].values\n",
    "        y_test = df[df[splitting_feature]==test_split_value].reset_index()[target].values\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "  \n",
    "#df = get_non_redundant_df(df, 'PTS_Result', 0.5) \n",
    "       \n",
    "X_train, y_train, X_test, y_test = feature_based_train_test_split(df, 'Won_Result', 'season', '2019-20')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.to_flat_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data/preprocessed/non_redundant_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Clusterize\n",
    "\n",
    "\n",
    "X_train_sc, X_test_sc = StandardScaler().fit_transform(X_train), StandardScaler().fit_transform(X_test)\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA(n_components=10)\n",
    "principalComponents_train = pca.fit_transform(X_train_sc)\n",
    "principalComponents_test = pca.fit_transform(X_test_sc)\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features)\n",
    "\n",
    "# Save components to a DataFrame\n",
    "PCA_components_train = pd.DataFrame(principalComponents_train)\n",
    "PCA_components_test = pd.DataFrame(principalComponents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PCA_components_train[0], PCA_components_train[1], alpha=.1, color='black')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PCA_components_test[0], PCA_components_test[1], alpha=.1, color='black')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(PCA_components.iloc[:,:3])\n",
    "\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
